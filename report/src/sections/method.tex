\section{Method}

Different architectures can be adopted to design an agent program implementing the underlying agent function mapping from percept histories to actions. Given the necessity to operate in a partially observable and dynamic environment in a proactive fashion, we based our implementation on the Belief-Desire-Intention (BDI) framework. By explicitly representing the agent's beliefs, desires and intentions, the BDI model allows the agent to reason about its environment, its goals and its own internal state, and to plan and act accordingly.

The pseudo-code of Algorithm~\ref{alg:agent} provides an high-level overview of the main parts of the agent program. After initializing its belief base (denoted as B) and its goals (denoted as D) based on its initial percepts, the agent enters the main control loop. At each iteration, the agent updates its set of beliefs based on its current observation of the surrounding environment and on the observations made by the agents cooperating with it. The newly updated belief base is then used to revise the agent's desires that are promptly exchanged with the other member of its team. Based on its own internal state and on the desires of the other agents, the agent then select from its set of desires the one to pursue. The chosen intention together with the current knowledge of the environment is then used to formulate a plan (i.e. a sequence of actions) to perform in order to achieve the intention. Finally, the agent executes the first action of the plan, updates its belief base based on the effects of the action. Given the highly dynamic nature of the environment, a cautious approach is adopted, where the agent never commits to execute a plan in its entirety, but instead re-evaluates its desires and intentions after each action.

\begin{algorithm}
    \caption{Agent control loop}
    \label{alg:agent}
    \begin{algorithmic}
        \item \textbf{Input:} $B_0$ (initial belief base)
        \State $B \gets$ $B_0$
        \State $D \gets$ options($B$)
        \While{true}
        \State perceive $o$
        \State $o \gets$ exchangeObservations($o$)
        \State $B \gets$ update($B$, $o$)
        \State $D \gets$ revise($D$, $B$)
        \State $D \gets$ exchangeDesires($D$)
        \State $I \gets$ select($B$, $D$)
        \State $P \gets$ plan($B$, $I$)
        \State $\alpha \gets$ head($P$)
        \State $r \gets$ execute($\alpha$)
        \State $B \gets$ update($B$, $r$)
        \EndWhile
    \end{algorithmic}
\end{algorithm}

Before delving into the details of the implementation, some clarifications need to be made. While for the sake of simplicity we have presented the agent conrol loop as a sequential process, in practice most of the operations are performed concurrently. Indeed, this is a paramount requirement in a highly dynamic environment such as the one of the Deliveroo game, where changings in the environment can happen even while the agent is reasoning, planning and acting. Thus, all perceptions, communications and updates are performed asynchronously and in an event-driven fashion such that updates to the belief base and the desires are performed as soon as new information is available.

\subsection{Team Communication}

As stated in the introduction, not all agents in the environment are adversarial. In fact, agents may form teams and cooperate with each other to better achieve their goals. To allow the formation of teams and the exchange of information among the agents (necessary for an effective and productive cooperation), a communication protocol has been implemented.

The protocol is built upon two communication primitives made available to the agents by the engine: \texttt{shout} and \texttt{say} (a third primitive, \texttt{ask}, is also available, but it has not been used in our implementation). The \texttt{shout} primitive allows the agent to broadcast a message to all the agents in the environment, while the \texttt{say} primitive allows the agent to send a message to a specific agent. Even though the provided communication channels are unreliable (i.e. there is no guarantee about when or if the message will be received), no reliability layer has been implemented on top of them, as the agents are designed to be robust to temporary communication failures. In fact, temporary inconsistencies in the belief bases of the team members should not impact the overall performance of the team in the long run.

Since agents do not know in advance which other agents are in the environment and which of them are adversarial or cooperative, the first step of the communication protocol is to establish the identity of the agents and to form teams. To this end, each agent broadcasts a \texttt{hello} message to all the agents in the environment. Since agents may appear at any time during the game, the \texttt{hello} message is periodically broadcasted by the agent. To allow the receiving agent to verify that the sender of the message is indeed a legitimate agent, the \texttt{hello} message contains the id of the sender encrypted using the AES-256 algorithm with a secret key known only to the agents of the same team. Upon receiving a \texttt{hello} message, the receiving agent can verify whether the decrypted id matches the id of the sender and, if so, it can add the sender to its set of cooperating agents. In this way, even if a malicious agent intercepts the \texttt{hello} message and tries to replay it, the receiving agent will notice that the id of the sender does not match the decrypted id and will discard the message.

The other part of the communication protocol regards the exchange of information among the agents in the same team. To avoid adversarial agents from exploiting the information exchanged among the team members, the following messages are not broadcasted to all the agents in the environment, but are instead sent only to the agents in the same team. Moreover, to avoid malicious agents from injecting false information in the belief bases of the team members, messages sent by non-cooperating agents are ignored. To this messages no encryption is applied since the \texttt{say} primitive should already guarantee that the message is received only by the intended recipient.

The exhange of information regards three main aspects: the observations of the environment, the desires of the agents, and the position of the agents.

The observations of the environment are the main source of information for the agents to update their belief bases (see Section~\ref{sec:belief-base} for more details). Each time an agent receives a new observation of the environment, it relays the observation to the other agents in the team. By doing so, the belief bases of the agents in the same team are kept in sync and their knowledge of the environment is both more accurate and more complete. One could argue that it would be better to only relay updates to the belief base, rather than each observation (which are much more frequent). However, we noticed that this leads to a higher risk of inconsistencies since temporary inaccuracies in the belief base of one agent will be propagated to all the other agents in the team.

As for the observations, each time an agent updates its desires, it relays the new desires to the other agents in the team. In this way, the agents in the team can coordinate their actions and not interfere with each other (e.g. by not picking up the same parcels) to better achieve their goals. Finally, each time the position of an agent changes, it relays its new position to the other agents in the team.

\subsection{Belief base}
\label{sec:belief-base}

In the BDI model, the belief base represents the information that the agent has about the world. In the context of the Deliveroo game, the informational state of the agent includes both the ground truth facts about the environment and the agent's own beliefs about the current state of the world. The former includes the topology of the map, the location of the pickup and delivery locations, and other parameters controlling the game dynamics. The latter includes the agent's beliefs about the state of the parcels and the other agents, as well as the agent's own state (e.g. its current location and its membership to a team). While the ground truth facts are fixed and given as input to the agent when the game starts, the agent's beliefs require to be constantly updated based on the received observations so as to mantain the most accurate representation of the world on which to reason and plan.

Hereafter, we describe the main assumptions and design choices made to keep track of the changes in the environment and to update the belief base accordingly.

\paragraph*{Parcels}
One of the main components of the belief base is the set of parcels that the agent can pickup (that is, the parcels that are not yet picked up by any agent). Each parcel is represented as a tuple $(p, l, t, v)$, where $p$ is a unique identifier, $l$ is the location of the parcel, $t$ is the time at which the parcel was first observed, and $v$ is the value of the parcel at time $t$ (by knowing the time at which the parcel was first observed, the agent can estimate the current value of the parcel based on its decay rate).

Each time the agent receives a new observation (from its sensors or from the sensors of the other team members), it updates the set of parcels in its belief base accordingly. In particular, if the agent observes a free parcel that it was not aware of, it adds the parcel to its belief base. If the agent observes a parcel that it was already aware of but in a different location or now picked up by another agent, it updates its location or removes it from its belief base accordingly. Given that the agent can only perceive its surroundings within a certain radius, the state of the parcels that are not within the team's perception range can not be determined with certainty. In such case, we decided to mark as no longer free only those parcels whose last observed location is inside the perception range but that are not currently observed by any agent in the team.

\paragraph*{Agents}
Tracking the state of the other agents is also a crucial part as their positions and intentions can greatly affect the agent's own plans. Here a distinction must be made between the agents in the same team and the adversarial agents. Tracking the state of the cooperating agents is straightforward, as their position and desires are directly communicated by the agents themselves. The only additional information that the agent needs to keep track of is the last time at which the agent has sent a message to the team. This is necessary to detect when an agent has been inactive for too long and to remove it from the team.

On the other hand, keeping the belief base up-to-date with the state of the adversarial agents is more challenging as we can only rely on the observations made by the agent itself and its team. Each of the adversarial agents is represented as a tuple $(a, l, t, r)$, where $a$ is a unique identifier, $l$ is the last observed location of the agent, $t$ is the time at which the agent was last observed, and $r$ is a boolean flag indicating whether the agent is considered a random agent. Note that by random agent we do not necessarily mean an agent that moves randomly (i.e. without any specific goal), but an agent whose behavior does not seem to be rational (i.e. whose actions do not seem to be aimed at maximizing its reward).

Each time the agent receives a new observation, it updates the set of adversarial agents in its belief base accordingly. In particular, if the agent observes a new agent, it adds the agent to its belief base, otherwise it updates the last observed location and the last observed time of the agent. Given that the agent can only perceive its surroundings within a certain radius, the state of the adversarial agents that are not within the team's perception range can not be determined with certainty. In such case, since agents are much more dynamic than parcels, we decided to mark as undefined the position of all agents that have not been observed for a certain amount of time. This is done to avoid the agent to make decisions based on outdated information.

To assess whether an agent is randomly moving, a simple heuristic has been defined. If from the first time the agent was observed to the last time the agent was observed its observed score is below a certain threshold, the agent is considered a random agent. The threshold is defined as the expected reward a greedy agent would have obtained in the same time span. This is computed in the following way:

\begin{equation*}
    \mathbb{E}_{\text{greedy}} = \frac{\mathbb{E}_{\texttt{distance}}}{\mu_{\texttt{distance}}} \cdot \frac{\mu_{\texttt{reward}}}{\texttt{numSmartAgents}}
\end{equation*}

where $\mathbb{E}_{\texttt{distance}}$ is the expected distance covered by the agent in the time span, $\mu_{\texttt{distance}}$ is the average distance between parcels in the map, $\mu_{\texttt{reward}}$ is the average reward of the parcels, and \texttt{numSmartAgents} is the number of agents in the environment that are not random agents.

Finally, note that here we do not make any assumption about the possible intentions of other agents. Indeed, modelling the behaviour of other agents is a complex and challenging task that may require learning-based approaches. Therefore, we preferred not to implement any partial model of the other agents' intentions, given that wrong assumptions about the actions of the other agents can lead to suboptimal decisions.

\subsection{Search}

This section describes the search algorithm used to update the set of desires of the agent and to select the intention to pursue.

\subsection{Planning}

Once the intention to pursue has been selected, the agent needs to formulate a plan to achieve the intention. In the case of the Deliveroo game, the plan consists of a sequence of move actions (up, down, left, right) to perform to reach the desired location. In other words, the planning problem can be seen as a shortest path problem on a two-dimensional grid.

Given that the computation of the shortest path is often required (after each action and during the whole search phase), recomputing the shortest path from scratch each time would be both inefficient and unnecessary. Therefore, considering also the fact that the map layout does not change except for the position of the agents, we decided to precompute the shortest path from each tile to each other tile of the map and to store the results in a look-up table. To this end, we decided to adopt the Seidel's algorithm~\parencite{seidel} to compute the shortest path between each pair of tiles. By treating the map as an undirected and unweighted graph (where each walkable tile is a node and each edge connects two adjacent tiles), the algorithm computes the shortest distance (that can be easily used to compute the shortest path) between each pair of nodes in $O(V^{\omega} log(V))$ time, where $V$ is the number of nodes and $\omega$ is the exponent of matrix multiplication. Given that the number of nodes in the map is relatively small, the precomputation of the lookup table can be performed in a reasonable amount of time that will be repaid by the speedup in the planning phase.

As stated above, the precomputed paths do not take into account the current state of the environment, that is the occupied positions. Thus, when the agent perform the first move of the plan, it may find the destination tile to be occupied by another agent. In such case, the agent needs to recompute the shortest path to the destination tile, taking into account the current state of the environment. To this end, we decided to leverage a PDDL planner to give us a viable path. In our case, the pddl domain is relatively simple as the only possible actions are up, down, left, right and the preconditions are that the destination tile isn't already taken by another agent. As the planning computation needs to be performed in real-time, rather that using the provided online planner (that would introduce further latency), we decided to use the Planutils tool \parencite{planutils} to locally run the FF PDDL planner.

To further speed up the planning phase, we also implemented a  solution based on the A* search algorithm \parencite{a*}. By further exploiting the precomputed distances as the heuristic function (an admissible and consistent heuristic), the A* search is guaranteed to find the shortest path in the most efficient way.

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{sections/figures/path-bottleneck.png}
    \caption{An example of path bottleneck. The tiles in yellow belong to the path bottleneck from the start position to the end tile.}
    \label{fig:path-bottleneck}
\end{figure}

The recomputed paths are then stored in a cache to be reused in the future. However, given the highly dynamic nature of the environment, an invalidation mechanism has been implemented to remove from the cache the paths no longer valid. To this end, the concept of path bottleneck is introduced. When moving from the current position to the destination tile, the set of all tiles that the agent must necessarily traverse to reach the destination in the shortest time is called the path bottleneck (see Figure~\ref{fig:path-bottleneck}). In other words, the path bottleneck is equal to the intersection of all the shortest paths from the current position to the destination. Given such definition, it is easy to see that the path recomputation is necessary only when the path bottleneck is no longer free. Thus, as soon as the path bottleneck becomes free again, the recomputed path can be discarded and the Seidel's cache can be used again.
